{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "import GPflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "\n",
    "# R magic\n",
    "import rpy2\n",
    "\n",
    "\n",
    "# the following lines will allow us to convert between Pandas DataFrames and R DataFrames\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "from rpy2.robjects.conversion import ri2py\n",
    "\n",
    "# this loads the R magic extension\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_prior = np.genfromtxt(fname = \"data/for_composititional_analysis_prior.csv\", \n",
    "                     delimiter = ',',\n",
    "                     usecols = (1,2,3,4,5),\n",
    "                     skip_header = 1,\n",
    "                     dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_posterior =  np.genfromtxt(fname = \"data/for_composititional_analysis_posterior.csv\", \n",
    "                                 delimiter = ',',\n",
    "                                 usecols = (1,2,3,4,5,6),\n",
    "                                 skip_header = 1,\n",
    "                                 dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xpredictions = np.linspace(31, 365*4, int(365*4-31+1))[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transformation of array to matrix\n",
    "def array_to_matrix(x):\n",
    "    X = []\n",
    "    for i in range(len(x)):\n",
    "        X.append([float(x[i])])\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes\n",
    "Docs:\n",
    "- [GP Regression](http://gpflow.readthedocs.io/en/latest/notebooks/regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute(X, Y, kernel_name):\n",
    "    # Get kernel\n",
    "    kernel = get_new_kernel(kernel_name)\n",
    "    \n",
    "    model = GPflow.gpr.GPR(X, Y, kern = kernel)\n",
    "    \n",
    "    white_added = False\n",
    "    second_exception = False\n",
    "    \n",
    "    try:\n",
    "        model.optimize()\n",
    "    except:\n",
    "        # Add white kernel\n",
    "        white_added = True\n",
    "        \n",
    "        w = GPflow.kernels.White(1, variance = 0.05)\n",
    "        w.variance.fixed = True\n",
    "        model = GPflow.gpr.GPR(X, Y, kern = kernel + w)\n",
    "        \n",
    "        try:\n",
    "            print('Adding White Kernel to', kernel_name)\n",
    "            model.optimize()\n",
    "        except:\n",
    "            second_exception = True\n",
    "            print('Exception caught computing', kernel_name)\n",
    "        \n",
    "    return {'model': model, 'white_added': white_added, 'second_exception': second_exception}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lml(model):\n",
    "    \"\"\"Log marginal likelihood of a GP\"\"\"\n",
    "    \n",
    "    try:\n",
    "        return model.compute_log_likelihood()\n",
    "    except:\n",
    "        print('Exception caught in lml')\n",
    "        return -999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(gps, X):\n",
    "    predictions = {}\n",
    "    \n",
    "    # For every GP, build predictions\n",
    "    for key in gps.keys():\n",
    "        \n",
    "        try:\n",
    "            mean, var = gps[key]['model'].predict_y(X)\n",
    "        except:\n",
    "            print('Exception caught in predict')\n",
    "            mean, var = np.array([0]), np.array([0])\n",
    "        \n",
    "        predictions[key] = {'mean': mean.tolist(), \n",
    "                            'var': var.tolist()}\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_kernel(kernel_string):\n",
    "    # Initial new non-optimized kernels\n",
    "    l = GPflow.kernels.Linear(1)\n",
    "    p = GPflow.kernels.PeriodicKernel(1)\n",
    "    r = GPflow.kernels.RBF(1)\n",
    "    \n",
    "    if   kernel_string == 'l': return l\n",
    "    elif kernel_string == 'p': return p\n",
    "    elif kernel_string == 'r': return r\n",
    "\n",
    "    elif kernel_string == 'l+r': return  l+r\n",
    "    elif kernel_string == 'l+p': return  l+p\n",
    "    elif kernel_string == 'p+r': return  p+r\n",
    "\n",
    "    elif kernel_string == 'l*r': return  l*r\n",
    "    elif kernel_string == 'l*p': return  l*p\n",
    "    elif kernel_string == 'p*r': return  p*r\n",
    "\n",
    "    elif kernel_string == 'l+r+p': return l+r+p\n",
    "    elif kernel_string == 'l+r*p': return l+r*p\n",
    "    elif kernel_string == 'l*r+p': return l*r+p\n",
    "    elif kernel_string == 'l*p+r': return l*p+r\n",
    "    elif kernel_string == 'l*r*p': return l*r*p\n",
    "    \n",
    "    else: return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(Y):\n",
    "    std = np.std(Y)\n",
    "    mu = np.mean(Y)\n",
    "    \n",
    "    return ((Y - mu)/std)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gps(X, Y0):\n",
    "    #Y = normalize(Y0)\n",
    "    Y = Y0\n",
    "    \n",
    "    gps = {}\n",
    "\n",
    "    gps['l'] = compute(X, Y, 'l')\n",
    "    gps['p'] = compute(X, Y, 'p')\n",
    "    gps['r'] = compute(X, Y, 'r')\n",
    "\n",
    "    gps['l+r'] = compute(X, Y, 'l+r')\n",
    "    gps['l+p'] = compute(X, Y, 'l+p')\n",
    "    gps['p+r'] = compute(X, Y, 'p+r')\n",
    "\n",
    "    gps['l*r'] = compute(X, Y, 'l*r')\n",
    "    gps['l*p'] = compute(X, Y, 'l*p')\n",
    "    gps['p*r'] = compute(X, Y, 'p*r')\n",
    "\n",
    "    gps['l+r+p'] = compute(X, Y, 'l+r+p')\n",
    "    gps['l+r*p'] = compute(X, Y, 'l+r*p')\n",
    "    gps['l*r+p'] = compute(X, Y, 'l*r+p')\n",
    "    gps['l*p+r'] = compute(X, Y, 'l*p+r')\n",
    "    gps['l*r*p'] = compute(X, Y, 'l*r*p')\n",
    "    \n",
    "    return gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_lmls(models):\n",
    "    lmls = {}\n",
    "    for key in models.keys():\n",
    "        e = {\n",
    "            'lml': lml(models[key]['model']),\n",
    "            'white_added': models[key]['white_added'],\n",
    "            'second_exception': models[key]['second_exception']\n",
    "            }\n",
    "        lmls[key] = e\n",
    "        \n",
    "    return lmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gps_to_string(gps):\n",
    "    strings = {}\n",
    "    for key in gps.keys():\n",
    "        strings[key] = str(gps[key])\n",
    "        \n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_max(d):\n",
    "    maxval = max(d.values())\n",
    "    keys = [k for k,v in d.items() if v==maxval]\n",
    "    return keys, maxval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'model': <GPflow.gpr.GPR object at 0x0000021BCD2D96D8>, 'white_added': False, 'second_exception': False}\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_prior['gpss']['1']['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, filename, new_format):\n",
    "    \n",
    "    if not new_format:\n",
    "        with open('output/' + filename + '.json', 'w') as fp:\n",
    "            json.dump(results, fp)\n",
    "            \n",
    "    else:\n",
    "        #lmls: pid, composition, lml, white_added, second_exception\n",
    "        with open('output/' + filename + '_lmls.csv', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            \n",
    "            #title\n",
    "            writer.writerow(['pid', 'composition', 'lml', 'white_added', 'second_exception'])\n",
    "            \n",
    "            for pid in results_prior['lmls']:\n",
    "                for composition in results_prior['lmls'][pid]:\n",
    "                    writer.writerow([\n",
    "                                     pid, \n",
    "                                     composition, \n",
    "                                     results_prior['lmls'][pid][composition]['lml'],\n",
    "                                     results_prior['lmls'][pid][composition]['white_added'],\n",
    "                                     results_prior['lmls'][pid][composition]['second_exception']\n",
    "                                    ])\n",
    "        \n",
    "        #Xpredictions #predictions\n",
    "        with open('output/' + filename + '_predictions.csv', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            \n",
    "            #title\n",
    "            writer.writerow(['pid', 'composition', 'Xpredictions', 'predictions_mean', 'predictions_var'])\n",
    "            \n",
    "            for pid in results_prior['predictions']:\n",
    "                for composition in results_prior['predictions'][pid]:\n",
    "                    for index, element in enumerate(results_prior['predictions'][pid][composition]['mean']):\n",
    "                        writer.writerow([\n",
    "                                         pid, \n",
    "                                         composition, \n",
    "                                         results_prior['Xpredictions'][index][0],\n",
    "                                         results_prior['predictions'][pid][composition]['mean'][index][0],\n",
    "                                         results_prior['predictions'][pid][composition]['var'][index][0]\n",
    "                                        ])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Gaussian Process Models for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gps_for_dataset(dataset, Xpredictions=Xpredictions):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ids = np.unique(dataset['f0'])\n",
    "    \n",
    "    gpss_objects = {}\n",
    "    gpss = {}\n",
    "    predictions = {}\n",
    "    lmls = {}\n",
    "    maxs = {}\n",
    "    \n",
    "    for i in ids:\n",
    "        print(i)\n",
    "        # Filter the relevant data\n",
    "        filtered_data = dataset[dataset['f0'] == i]\n",
    "        \n",
    "        # Get X and Y\n",
    "        X = array_to_matrix(filtered_data['f3'])\n",
    "        Y = array_to_matrix(filtered_data['f4'])\n",
    "        \n",
    "        # Compute GPs\n",
    "        gps = compute_gps(X, Y)\n",
    "        print('Compute OK')\n",
    "        \n",
    "        # Calculate the predictions of the GP given the initial data\n",
    "        \n",
    "        # Find the best fitting GP\n",
    "        likelihoods = compute_lmls(gps)\n",
    "        #best = dict_max(likelihoods)\n",
    "        print('LMLs OK')\n",
    "        \n",
    "        # Make predictions\n",
    "        gps_predictions = predict(gps, Xpredictions)\n",
    "        print('Predictions OK')\n",
    "        \n",
    "        # Save\n",
    "        i = str(i)\n",
    "        #gpss_objects[i] = gps\n",
    "        #gpss[i] = gps_to_string(gps) # The GP parameters\n",
    "        predictions[i] = gps_predictions\n",
    "        lmls[i] = likelihoods\n",
    "        #maxs[i] = best\n",
    "        \n",
    "\n",
    "    print('Minutes:', str(round((time.time() - t0) / 60)))\n",
    "        \n",
    "    return {\n",
    "            #'gpss_objects': gpss_objects, #Actual objects\n",
    "            #'gpss': gpss, \n",
    "            'Xpredictions': Xpredictions.tolist(), \n",
    "            'predictions': predictions, \n",
    "            'lmls': lmls, \n",
    "            #'maxs': maxs\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(X, Y, mean, var):\n",
    "    xx = Xpredictions\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X, Y, 'kx', mew=2)\n",
    "    plt.plot(xx, mean, 'b', lw=2)\n",
    "    plt.fill_between(xx[:,0], mean[:,0] - 2*np.sqrt(var[:,0]), mean[:,0] + 2*np.sqrt(var[:,0]), color='blue', alpha=0.2)\n",
    "    plt.xlim(31, 365*4)\n",
    "    #plt.ylim(-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_predictions(results, data, target_id, target_kernel):\n",
    "\n",
    "    dat = data[data['f0'] == target_id]\n",
    "\n",
    "    X = array_to_matrix(dat['f3'])\n",
    "    Y = normalize(array_to_matrix(dat['f4']))\n",
    "\n",
    "    mean = np.array(results['predictions'][str(target_id)][target_kernel]['mean'])\n",
    "    var = np.array(results['predictions'][str(target_id)][target_kernel]['var'])\n",
    "\n",
    "    plot(X, Y, mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Gaussian Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debug_filtering(dataset):\n",
    "    #dataset = dataset[dataset['f0'] == 59]\n",
    "    dataset = dataset[dataset['f0'] < 2 ]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scenario(dataset, scenario):\n",
    "    return dataset[dataset['f2'] == scenario]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'\"FB Friends\"', b'\"Gym members\"', b'\"Rain\"', b'\"Salary\"',\n",
       "       b'\"Sales\"', b'\"Temperature\"'], \n",
       "      dtype='|S13')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data_prior['f2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Adding White Kernel to l*r\n",
      "Adding White Kernel to l*p\n",
      "Compute OK\n",
      "LMLs OK\n",
      "Predictions OK\n",
      "Minutes: 1\n"
     ]
    }
   ],
   "source": [
    "dataset = debug_filtering(data_prior)\n",
    "results_prior = compute_gps_for_dataset(dataset)\n",
    "save_results(results_prior, 'cross-validation-prior/results_prior_test', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = debug_filtering(data_prior)\n",
    "results_prior = compute_gps_for_dataset(dataset)\n",
    "save_results(results_prior, 'cross-validation-prior/results_prior_test', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior condition (only evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset = debug_filtering(data_posterior)\n",
    "\n",
    "# Only the evidence\n",
    "#dataset = dataset[dataset['f3'] < (365-31+1)]\n",
    "\n",
    "#results_posterior = compute_gps_for_dataset(dataset)\n",
    "#save_results(results_posterior, 'results_posterior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_predictions(results_posterior, data_posterior, 8, 'l+p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_predictions(results_prior, data_prior, 9, 'l+r*p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "source(\"tools.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output/results_prior.json', 'r') as fp:\n",
    "    results_prior = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output/results_prior_maxs.json', 'r') as fp:\n",
    "    results_prior_maxs = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('output/results_posterior.json', 'r') as fp:\n",
    "    results_posterior = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aux = pd.DataFrame(results_posterior['maxs']).T\n",
    "#aux.columns = ['kernel', 'lml']\n",
    "\n",
    "#for i, row in aux.iterrows():\n",
    "#    aux.set_value(i, 'kernel', row['kernel'][0])\n",
    "\n",
    "#aux.to_csv('output/results_posterior_maxs.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aux = pd.DataFrame(results_prior['lmls']).T\n",
    "#aux.to_csv('output/lmls_prior.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "aux = pd.DataFrame(results_posterior['lmls']).T\n",
    "aux.to_csv('output/lmls_posterior.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "kernels <- c(\"l\", \"p\", \"r\", \"l+p\", \"l+r\", \"p+r\", \"l*r\", \"l*p\", \"p*r\", \"l+r+p\", \"l+r*p\", \"l*r+p\", \"l*p+r\", \"l*r*p\")\n",
    "\n",
    "get_proportions <- function(res_x) {\n",
    "    total <- length(res_x[1][[1]])\n",
    "\n",
    "    props <- res_x %>% \n",
    "                group_by(kernel) %>%\n",
    "                summarize(proportion = length(kernel)/total,\n",
    "                          lo_ci = prop.test(x=length(kernel), n=total, conf.level=0.95, correct = FALSE)$conf[1],\n",
    "                          hi_ci = prop.test(x=length(kernel), n=total, conf.level=0.95, correct = FALSE)$conf[2])\n",
    "    \n",
    "    props$kernel <- factor(props$kernel, levels=kernels)\n",
    "    \n",
    "    # Adding zeros\n",
    "    for (k in kernels){\n",
    "        # If the kernel is NOT present\n",
    "        if (sum(props$kernel == k) == 0){         \n",
    "            new_row <- c(k, 0, 0, 0)\n",
    "            \n",
    "            props <- rbind(props, new_row)\n",
    "        } \n",
    "    }\n",
    "    \n",
    "    # Reorder factors\n",
    "    #props$kernel <- factor(props$kernel, levels=c(\"l\", \"p\", \"r\", \"l+p\", \"l+r\", \"p+r\", \"l*r\", \"l*p\", \"p*r\", \"l+r+p\", \"l+r*p\", \"l*r+p\", \"l*p+r\", \"l*r*p\"))\n",
    "    \n",
    "    props$proportion <- as.numeric(props$proportion)\n",
    "    \n",
    "    return (props)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "plot_proportions <- function(prop_data, title, hide_x=FALSE, hide_y=FALSE) {\n",
    "    plot <- prop_data %>%\n",
    "                ggplot(aes(x=kernel, y=proportion)) + \n",
    "                    geom_bar(stat=\"identity\") +\n",
    "                    #ylim(0, 0.5) +\n",
    "                    coord_cartesian (ylim=c(0,0.5)) +\n",
    "                    labs(title = title) +\n",
    "                    ggthemes::theme_few() +\n",
    "                    xlab(\"Kernel composition\") + ylab(\"Proportion\") +\n",
    "                    geom_errorbar(aes(ymin=as.numeric(lo_ci), ymax=as.numeric(hi_ci)),\n",
    "                                      width=.3,                   \n",
    "                                      position=position_dodge(.9)) +\n",
    "                    theme(axis.text.x = element_text(angle = 90,  vjust = 0.5, hjust=0),\n",
    "                          text = element_text(size=12, family=\"serif\"),\n",
    "                          plot.title = element_text(hjust = 0.5))\n",
    "    \n",
    "    if(hide_x){\n",
    "        plot <- plot + theme(axis.title.x=element_blank())\n",
    "    }\n",
    "    if(hide_y){\n",
    "        plot <- plot + theme(axis.title.y=element_blank())\n",
    "    }\n",
    "    \n",
    "    return(plot)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportions of best fitting kernel composition in the Prior condition, per scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxs_prior = pd.DataFrame(data=results_prior['maxs']).T\n",
    "#maxs_prior = pd.DataFrame(data=results_posterior['maxs']).T\n",
    "maxs_prior.columns = ['kernel', 'lml']\n",
    "\n",
    "for i in maxs_prior.index:\n",
    "    maxs_prior['kernel'][i] = maxs_prior['kernel'][i][0]\n",
    "    \n",
    "maxs_prior['id'] = maxs_prior.index.values.tolist()\n",
    "\n",
    "maxs_prior_i = maxs_prior['id'].values.tolist()\n",
    "maxs_prior_k = maxs_prior['kernel'].values.tolist()\n",
    "maxs_prior_l = maxs_prior['lml'].values.tolist()\n",
    "\n",
    "#maxs_posterior =  = pd.Series(results_posterior['maxs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "data_prior <- read_csv(\"data/for_composititional_analysis_prior.csv\")\n",
    "\n",
    "dict_prior <- data_prior %>% \n",
    "                        group_by(id, scenario) %>%\n",
    "                        summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -i maxs_prior_i,maxs_prior_k,maxs_prior_l\n",
    "\n",
    "maxs_prior <- data.frame(id=maxs_prior_i,kernel=maxs_prior_k,lml=maxs_prior_l)\n",
    "\n",
    "maxs_prior <- merge(x = maxs_prior, y = dict_prior, by = c(\"id\", \"id\"), all.x = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "maxs_prior_temperature <- maxs_prior %>% filter(scenario == \"Temperature\")\n",
    "maxs_prior_rain <- maxs_prior %>% filter(scenario == \"Rain\")\n",
    "maxs_prior_sales <- maxs_prior %>% filter(scenario == \"Sales\")\n",
    "maxs_prior_gym <- maxs_prior %>% filter(scenario == \"Gym members\")\n",
    "maxs_prior_salary <- maxs_prior %>% filter(scenario == \"Salary\")\n",
    "maxs_prior_fb <- maxs_prior %>% filter(scenario == \"FB Friends\")\n",
    "\n",
    "prop_temperature <- get_proportions(maxs_prior_temperature)\n",
    "prop_rain <- get_proportions(maxs_prior_rain)\n",
    "prop_sales <- get_proportions(maxs_prior_sales)\n",
    "prop_gym <- get_proportions(maxs_prior_gym)\n",
    "prop_salary <- get_proportions(maxs_prior_salary)\n",
    "prop_fb <- get_proportions(maxs_prior_fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "p1 <- plot_proportions(prop_temperature, \"Temperature\", hide_x=TRUE)\n",
    "p2 <- plot_proportions(prop_rain, \"Rain\", hide_x=TRUE)\n",
    "p3 <- plot_proportions(prop_sales, \"Sales\", hide_x=TRUE, hide_y=TRUE)\n",
    "p4 <- plot_proportions(prop_gym, \"Gym members\", hide_y=TRUE, hide_x=TRUE)\n",
    "p5 <- plot_proportions(prop_salary, \"Salary\", hide_x=TRUE, hide_y=TRUE)\n",
    "p6 <- plot_proportions(prop_fb, \"FB Friends\", hide_y=TRUE, hide_x=TRUE)\n",
    "\n",
    "svg(\"Images/kernels_prior.svg\", width=8, height=4)\n",
    "multiplot(p1, p2, p3, p4, p5, p6, cols=3)\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the best kernel composition SSE versus those of `l` and `r`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posterior_curve(cid):\n",
    "    cid = int(cid)\n",
    "    dataset = data_posterior\n",
    "    \n",
    "    # Filter the relevant data\n",
    "    filtered_data = dataset[dataset['f0'] == cid]\n",
    "    \n",
    "    # Get X and Y\n",
    "    x = filtered_data['f3']\n",
    "    y = filtered_data['f4']\n",
    "    \n",
    "    df = pd.DataFrame([x, y]).T\n",
    "    \n",
    "    df.columns=[\"x\", \"y\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posterior_prediction(cid, kernel_name):\n",
    "    # Get the target values\n",
    "    x = Xpredictions\n",
    "    \n",
    "    y = results_posterior['predictions'][cid][kernel_name]['mean']\n",
    "    \n",
    "    y_var = results_posterior['predictions'][cid][kernel_name]['var']\n",
    "    \n",
    "    # Squeeze the matrices\n",
    "    x = np.squeeze(x); y = np.squeeze(y); y_var = np.squeeze(y_var); \n",
    "    \n",
    "    df = pd.DataFrame([x, y, y_var]).T\n",
    "    \n",
    "    df.columns=[\"x\", \"y\", \"y_var\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_y_value(df, x):\n",
    "    return df[df['x']==x]['y'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_SSE(true_dataframe, prediction_dataframe, minX = 365-31):\n",
    "    \n",
    "    df1 = true_dataframe[true_dataframe['x'] > minX]\n",
    "    df2 = prediction_dataframe[prediction_dataframe['x'] > minX]\n",
    "    \n",
    "    sse = 0\n",
    "    \n",
    "    for x in df1['x'] :\n",
    "        error = get_y_value(df1, x) - get_y_value(df2, x)\n",
    "        sse += (error*error)\n",
    "        \n",
    "    # Root mean squared deviation\n",
    "    rmsd = np.sqrt(sse / len(df1))\n",
    "    \n",
    "    # Normalized\n",
    "    return rmsd / (np.max(df1['y']) - np.min(df1['y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"cid\", \"sse_l\", \"sse_r\", \"sse_best\"]\n",
    "\n",
    "sses = pd.DataFrame(columns = columns)\n",
    "\n",
    "for cid in np.unique(data_prior['f0']):\n",
    "    \n",
    "    if cid != 533:\n",
    "        cid = str(cid)\n",
    "        \n",
    "        # Get the best kernel for the curve\n",
    "        best_kernel_name = results_prior['maxs'][cid][0][0]\n",
    "\n",
    "        # Gest the predictions of the l, r, and best kernel\n",
    "        prediction_l = get_posterior_prediction(cid, 'l')\n",
    "        prediction_r = get_posterior_prediction(cid, 'r')\n",
    "        prediction_best = get_posterior_prediction(cid, best_kernel_name)\n",
    "\n",
    "        # Get the actual values in the posterior condition\n",
    "        prediction = get_posterior_curve(cid)\n",
    "\n",
    "        # Calculate the Sum of squared errors\n",
    "        sse_l = compute_SSE(prediction, prediction_l)\n",
    "        sse_r = compute_SSE(prediction, prediction_r)\n",
    "        sse_best = compute_SSE(prediction, prediction_best)\n",
    "\n",
    "        results = pd.DataFrame([cid, sse_l, sse_r, sse_best]).T\n",
    "        results.columns = [\"cid\", \"sse_l\", \"sse_r\", \"sse_best\"]\n",
    "\n",
    "        sses = sses.append(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.99):\n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * sp.stats.t._ppf((1+confidence)/2., n-1)\n",
    "    return m, m-h, m+h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NRMSD    Mean  -CI99  +CI99')\n",
    "\n",
    "print ('Linear:', np.round(mean_confidence_interval(sses['sse_l']),2))\n",
    "\n",
    "print ('RBF:   ', np.round(mean_confidence_interval(sses['sse_r']),2))\n",
    "\n",
    "print ('Best:  ', np.round(mean_confidence_interval(sses['sse_best']),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sse_l = sses['sse_l'].tolist()\n",
    "sse_r = sses['sse_r'].tolist()\n",
    "sse_best = sses['sse_best'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paired t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i sse_l,sse_r,sse_best\n",
    "\n",
    "t.test(sse_l, sse_r, paired=TRUE, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i sse_l,sse_r,sse_best\n",
    "\n",
    "t.test(sse_l, sse_best, paired=TRUE, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i sse_l,sse_r,sse_best\n",
    "\n",
    "t.test(sse_r, sse_best, paired=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -i sse_l,sse_r,sse_best\n",
    "\n",
    "sse_l = data.frame(kernel=\"l\", sse=sse_l)\n",
    "sse_r = data.frame(kernel=\"r\", sse=sse_r)\n",
    "sse_best = data.frame(kernel=\"best\", sse=sse_best)\n",
    "\n",
    "sses = rbind(sse_l, sse_r, sse_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "p <- ggplot(sses, aes(x=kernel, y=sse, group=kernel)) +\n",
    "        geom_point(alpha=0.1, aes(size=3)) +\n",
    "        xlab(\"Kernel composition\") + ylab(\"NRMSE\") +\n",
    "        theme(text = element_text(size=20, family=\"serif\"),\n",
    "            plot.title = element_text(hjust = 0.5),\n",
    "            legend.position=\"none\")\n",
    "\n",
    "#svg(\"Images/kernels_nmse.svg\")\n",
    "p\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "p <- ggplot(sses, aes(x=kernel, y=sse, group=kernel)) +\n",
    "        geom_point(alpha=0.1, aes(size=3)) +\n",
    "        xlab(\"Kernel composition\") + ylab(\"NRMSE\") +\n",
    "        ylim(0,60) + \n",
    "        theme(text = element_text(size=20, family=\"serif\"),\n",
    "            plot.title = element_text(hjust = 0.5),\n",
    "            legend.position=\"none\")\n",
    "\n",
    "#svg(\"Images/kernels_nmse2.svg\")\n",
    "p\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judgmental forecasting effects in compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid =  '1'\n",
    "best_kernel_name = results_prior_maxs[cid][0][0]\n",
    "str(best_kernel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "\n",
    "for cid in np.unique(data_prior['f0']):\n",
    "    \n",
    "    if cid != 533:\n",
    "        cid = str(cid)\n",
    "\n",
    "        # Best kernel\n",
    "        best_kernel_name = str(results_prior_maxs[cid][0][0])\n",
    "        prediction_best = get_posterior_prediction(cid, best_kernel_name)\n",
    "\n",
    "        # Get the actual values in the posterior condition\n",
    "        prediction = get_posterior_curve(cid)\n",
    "        \n",
    "        # Filter the Best Kernel predictions; one every 5 days and with a max\n",
    "        prediction_best = prediction_best[(prediction_best['x'] % 5 == 1) & (prediction_best['x'] <= max(prediction['x']))]\n",
    "        \n",
    "        # Adding columns\n",
    "        prediction_best = prediction_best.assign(cid=cid, kernel_prediction=True)\n",
    "        \n",
    "        prediction = prediction.assign(cid=cid, kernel_prediction=False)\n",
    "        \n",
    "        # Concatenating to predictions dataframe\n",
    "        predictions = predictions.append(prediction_best, ignore_index=True)\n",
    "        predictions = predictions.append(prediction, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "data_posterior <- read_csv(\"data/for_composititional_analysis_posterior.csv\")\n",
    "\n",
    "dict_posterior <- data_posterior %>% \n",
    "                        group_by(id, scenario, condition) %>%\n",
    "                        summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -i predictions\n",
    "\n",
    "colnames(predictions) <- c(\"id\", \"kernel_prediction\", \"day\", \"value\", \"var\")\n",
    "predictions <- merge(x = predictions, y = dict_posterior, by = c(\"id\"), all.x = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "predictions %>% head\n",
    "\n",
    "write.csv(predictions, 'output/posterior_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is ready, let's graph the Judgmental forecasting phenomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "dat <- predictions %>% filter(kernel_prediction)\n",
    "readable_scenarios <- c(\"Temperature\", \"Rain\", \"Sales\", \"Gym members\", \"Salary\", \"FB Friends\")\n",
    "condition_names = c(\"Prior\", \"Posterior-Positive\", \"Posterior-Stable\", \"Posterior-Negative\")\n",
    "\n",
    "# Order on the plot\n",
    "dat$condition <- factor( dat$condition, \n",
    "                         levels = condition_names)\n",
    "\n",
    "dat$scenario <- factor( dat$scenario, \n",
    "                        levels = readable_scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORM DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "df <- dat %>%\n",
    "        group_by(condition, scenario, day) %>%\n",
    "        summarize(mean(value))\n",
    "\n",
    "df$slope_scale <- mapvalues(df$condition,\n",
    "                              from = c(\"Prior\", \"Posterior-Positive\", \"Posterior-Stable\", \"Posterior-Negative\"),\n",
    "                              to   = c(0, 1, 0, -1))\n",
    "\n",
    "df$range <- mapvalues(df$scenario,\n",
    "                              from = c(\"Temperature\", \"Rain\", \"Sales\", \"Gym members\", \"Salary\", \"FB Friends\"),\n",
    "                              to   = c( 40--10,        100-0,  5000-0,  50-0,          50-0,     1000-0))\n",
    "\n",
    "df$y_intercept <- mapvalues(df$scenario,\n",
    "                              from = c(\"Temperature\", \"Rain\", \"Sales\", \"Gym members\", \"Salary\", \"FB Friends\"),\n",
    "                              to   = c( 15,            30,     2500,    25,            20,       500))\n",
    "\n",
    "\n",
    "# Transformations to numbers\n",
    "df$slope_scale <- as.numeric(levels(df$slope_scale))[df$slope_scale]\n",
    "df$range <- as.numeric(levels(df$range))[df$range]\n",
    "df$y_intercept <- as.numeric(levels(df$y_intercept))[df$y_intercept]\n",
    "\n",
    "# Calculations\n",
    "df$last_point <- df$y_intercept + 0.05 * df$range * df$slope_scale * 4 \n",
    "\n",
    "df$slope <- (df$last_point - df$y_intercept) / ((365-31)-1)\n",
    "\n",
    "df$underlying_trend <- ((df$day-1) * df$slope + df$y_intercept)\n",
    "\n",
    "scaling <- df %>%\n",
    "                    group_by(condition, scenario) %>%\n",
    "                    summarize(y_intercept = unique(y_intercept), last_point= unique(last_point), range = unique(range))\n",
    "\n",
    "# Use the scaling object to scale the data\n",
    "norm_data <- merge(x = dat, y = scaling, by = c(\"condition\", \"scenario\"), all.x = TRUE)\n",
    "\n",
    "norm_data <- norm_data %>%\n",
    "                group_by() %>%\n",
    "                mutate(value_scaled = (value - y_intercept)/(range))\n",
    "# Normalize the data\n",
    "norm_data <- norm_data %>%\n",
    "                group_by() %>%\n",
    "                mutate(value_norm = scale(value_scaled))\n",
    "\n",
    "# Calculate the means and SDs\n",
    "means <- norm_data %>%\n",
    "            group_by() %>%\n",
    "            summarize(value_mean = mean(value_scaled),\n",
    "                      value_sd = sd(value_scaled))\n",
    "\n",
    "# Remove the scaled data\n",
    "norm_data <- norm_data %>% select(-value_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Scaled trend\n",
    "underlying_trend <- df %>%\n",
    "            group_by() %>%\n",
    "            mutate(underlying_trend_scaled = (underlying_trend - y_intercept)/(range))\n",
    "\n",
    "# Normalize trend\n",
    "underlying_trend <- underlying_trend %>%\n",
    "            mutate(underlying_trend_norm = ((underlying_trend_scaled - means$value_mean)/means$value_sd))\n",
    "\n",
    "underlying_trend <- underlying_trend %>%\n",
    "            select(condition, scenario, day, underlying_trend_norm)\n",
    "    \n",
    "    \n",
    "# Merge the trend into the normalized data\n",
    "norm_data <- merge(x=norm_data, y=underlying_trend, by=c(\"condition\", \"scenario\", \"day\"), all.x=TRUE)\n",
    "\n",
    "# Merging transforms the `value_norm` column into a matrix. This fixes that. No one knows why that happens.\n",
    "norm_data$value_norm <- c(norm_data$value_norm)\n",
    "\n",
    "underlying_trend$id <- 1\n",
    "\n",
    "\n",
    "norm_data <- norm_data %>%\n",
    "                mutate(abs_error = abs(underlying_trend_norm - value_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "grid_plot <- function(dplot, trend, res=30, type=1) {\n",
    "    \n",
    "    if(type == 1){\n",
    "        y = \"value\"\n",
    "        group = \"id\"\n",
    "        ylab = \"Value\"\n",
    "        trend_y = \"value\"\n",
    "    }\n",
    "    else if(type == 2){\n",
    "        y = \"value_norm\"\n",
    "        group = \"id\"\n",
    "        ylab = \"Scaled value\"\n",
    "        trend_y = \"underlying_trend_norm\"\n",
    "    }\n",
    "    \n",
    "    # Subsetting the data\n",
    "    #dplot <- subset(dplot, day %in% seq(min(dplot$day), max(dplot$day), res))\n",
    "    \n",
    "    # Plotting\n",
    "    p <- dplot %>%\n",
    "            ggplot(aes_string(x=\"day\", y=y, group=group)) + \n",
    "            geom_line(col=\"steelblue\", alpha=0.2) +\n",
    "            facet_grid(scenario~condition, scales=\"free_y\") +\n",
    "            theme_classic() + \n",
    "            xlab(\"Year\") + \n",
    "            ylab(ylab) +\n",
    "            geom_line(data=trend, aes_string(x=\"day\", y=trend_y), colour=\"black\") +\n",
    "            scale_x_continuous(breaks=c(0, 365, 365*2, 365*3), labels=c('Y1', 'Y2', 'Y3', 'Y4')) +\n",
    "            geom_vline(data=filter(trend, condition!=\"Prior\"), aes(xintercept=365-31), colour=\"steelblue\") +\n",
    "            theme(  text = element_text(size=12, family=\"serif\"),\n",
    "                    axis.line = element_line(colour=\"black\", size=0.1),\n",
    "                    panel.spacing.x = unit(0.5, \"lines\"),\n",
    "                    panel.spacing.y = unit(0.8, \"lines\"),\n",
    "                    panel.background = element_blank(),\n",
    "                    panel.grid.major = element_blank(), \n",
    "                    panel.grid.minor = element_blank(),\n",
    "                    panel.border = element_rect(colour=\"black\", fill=NA, size=0.1))\n",
    "        \n",
    "    return (p)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREND DAMPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot: trend damping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Filter out the prior condition and {temperature, rain} scenarios. Also filter out before year 1 and after year 4\n",
    "filtered_norm_data <- norm_data %>%\n",
    "        filter(condition != 'Prior', condition != 'Posterior-Stable', scenario != 'Temperature', scenario != 'Rain', day > 365-31, day <= 365*3 )\n",
    "\n",
    "filtered_underlying_trend <- underlying_trend %>%\n",
    "        filter(condition != 'Prior', condition != 'Posterior-Stable', scenario != 'Temperature', scenario != 'Rain', day > 365-31, day <= 365*3 )\n",
    "\n",
    "p <- grid_plot(filtered_norm_data, filtered_underlying_trend, type=2, res=5)\n",
    "\n",
    "#svg(\"Images/kernel_trend_damping.svg\", width=5, height=5)\n",
    "p\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Adding `damped` column\n",
    "norm_data <- norm_data %>%\n",
    "                mutate(damped = value_norm < underlying_trend_norm) \n",
    "\n",
    "norm_data$damped <- ifelse(norm_data$condition == \"Posterior-Positive\", norm_data$damped, !norm_data$damped)\n",
    "\n",
    "# Summarize overall scenarios\n",
    "damping_overall <- norm_data %>% \n",
    "                        filter(condition != 'Prior', condition != 'Posterior-Stable', scenario != 'Temperature', scenario != 'Rain', day > 365-31, day <= 365*3 ) %>%\n",
    "                        group_by(condition, day) %>%\n",
    "                        summarize(damped_proportion = sum(damped*1)/length(damped),\n",
    "                                  ci_lower = ifelse(damped_proportion<1, t.test(damped*1, conf.level=0.95)$conf.int[1], damped_proportion),\n",
    "                                  ci_upper = ifelse(damped_proportion<1, t.test(damped*1, conf.level=0.95)$conf.int[2], damped_proportion)) %>%\n",
    "                        mutate(group = 1)\n",
    "                                  \n",
    "# Summarize per day\n",
    "damping_detail <- norm_data %>%\n",
    "                            filter(condition != 'Prior', condition != 'Posterior-Stable', scenario != 'Temperature', scenario != 'Rain', day > 365-31, day <= 365*3 ) %>%\n",
    "                            group_by(condition, scenario, day) %>%\n",
    "                            summarize(damped_proportion = sum(damped*1)/length(damped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Reversing the conditions (design thing)\n",
    "dplot <- damping_overall\n",
    "dplot$condition <- factor(dplot$condition, levels = rev(levels(dplot$condition)))\n",
    "\n",
    "# Plotting\n",
    "p <- plot_with_ribbon(dplot, type=1)\n",
    "\n",
    "#svg(\"Images/kernel_trend_damping_overall_with_ci.svg\", width=6, height=2)\n",
    "p\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "dplot <- norm_data %>% \n",
    "            filter(condition != 'Prior', condition != 'Posterior-Stable', scenario != 'Temperature', scenario != 'Rain', day > 365-31, day <= 365*3 ) %>%\n",
    "            group_by(condition, day) %>%\n",
    "            summarize(mae = mean(abs_error),\n",
    "                      ci_lower = t.test(abs_error, conf.level=0.999)$conf.int[1],\n",
    "                      ci_upper = t.test(abs_error, conf.level=0.999)$conf.int[2]) %>%\n",
    "            mutate(group = 1)\n",
    "\n",
    "# Reversing the conditions (design thing)\n",
    "dplot$condition <- factor(dplot$condition, levels = rev(levels(dplot$condition)))\n",
    "\n",
    "# Plotting\n",
    "p <- plot_with_ribbon(dplot, type=2)\n",
    "\n",
    "#svg(\"Images/kernel_trend_damping_overall_mae_with_ci.svg\", width=6, height=2)\n",
    "p\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "ggsave(file=\"Images/kernel_trend_damping_overall_mae_with_ci.svg\", plot=p, width=10, height=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "dplot <- norm_data %>% \n",
    "            filter(condition != 'Prior', condition != 'Posterior-Stable', scenario != 'Temperature', scenario != 'Rain', day > 365-31, day <= 365*3 ) %>%\n",
    "            group_by(condition, scenario, day) %>%\n",
    "            summarize(mae = mean(abs_error),\n",
    "                      ci_lower = t.test(abs_error, conf.level=0.95)$conf.int[1],\n",
    "                      ci_upper = t.test(abs_error, conf.level=0.95)$conf.int[2]) %>%\n",
    "            mutate(group = 1)\n",
    "\n",
    "# Reversing the conditions (design thing)\n",
    "dplot$condition <- factor(dplot$condition, levels = rev(levels(dplot$condition)))\n",
    "\n",
    "p1 <- plot_with_ribbon(dplot %>% filter(scenario==\"Sales\"),       type=3,  hide_legend=TRUE, ylab=\"Mean absolute error\")\n",
    "p2 <- plot_with_ribbon(dplot %>% filter(scenario==\"Gym members\"), type=3,  hide_legend=TRUE)\n",
    "p3 <- plot_with_ribbon(dplot %>% filter(scenario==\"Salary\"),      type=3,  hide_legend=TRUE)\n",
    "p4 <- plot_with_ribbon(dplot %>% filter(scenario==\"FB Friends\"),  type=3,  hide_legend=TRUE)\n",
    "\n",
    "#svg(\"Images/kernels_trend_damping_scenarios_mae_with_ci.svg\", width=6, height=1.5)\n",
    "#multiplot(p1, p3, p2, p4, cols=4)\n",
    "#dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "plot_with_ribbon <- function(dplot, type, title=\"\", xlab=\"Year\", ylab=\"\", hide_legend=FALSE, group=\"condition\", legend_title=\"Condition\") {\n",
    "    \n",
    "    if(type == 1){\n",
    "        y = \"damped_proportion\"\n",
    "        ylab = \"Proportion of\\ndamped predictions\"\n",
    "    }\n",
    "    else if(type == 2){\n",
    "        y = \"mae\"\n",
    "        ylab = \"Mean absolute error\"\n",
    "    }\n",
    "    else if(type == 3){\n",
    "        y = \"mae\"\n",
    "        title = dplot$scenario[1]\n",
    "    }\n",
    "    else if(type == 4){\n",
    "        y = \"mae\"\n",
    "        legend_title = \"Noise Group\"\n",
    "        group = \"noise\"\n",
    "        title = dplot$condition[1]\n",
    "    }\n",
    "    else if(type==5 || type==6){\n",
    "        y = \"mae\"\n",
    "        legend_title = \"Noise Group\"\n",
    "        group = \"noise\"\n",
    "    }\n",
    "    \n",
    "    p <- ggplot(data=dplot, aes_string(x=\"day\", y=y, group=group, colour=group)) +\n",
    "            geom_line(size=1) +\n",
    "            geom_ribbon(aes(ymin=ci_lower, ymax=ci_upper), alpha=0.2) +\n",
    "            #scale_x_continuous(limits +\n",
    "            #coord_cartesian(ylim = c(0.65, 1.00), xlim = c(365-31, 365*2-10)) +\n",
    "            labs(colour = legend_title) +\n",
    "            xlab(xlab) + ylab(ylab) + ggtitle(title) +\n",
    "            scale_x_continuous(breaks=c(0, 365, 365*2, 365*3), labels=c('Y1', 'Y2', 'Y3', 'Y4')) +\n",
    "            theme_bw() + \n",
    "            #ggthemes::theme_few() + \n",
    "            theme(text = element_text(size=12, family=\"serif\")) +\n",
    "            #theme(legend.position=c(0.82, 0.40)) +\n",
    "            theme(plot.title = element_text(hjust = 0.5)) #+\n",
    "            #ggthemes::scale_color_solarized()\n",
    "    \n",
    "        \n",
    "    if(type == 3){\n",
    "        p <- p + coord_cartesian(ylim = c(0, 3))\n",
    "    }\n",
    "    else if(type == 4) {\n",
    "        p <- p + theme(legend.position=c(1.0, 0.4))\n",
    "    }\n",
    "    else if(type == 5) {\n",
    "        p <- p + coord_cartesian(ylim = c(0, 4))\n",
    "    }\n",
    "    else if(type == 6) {\n",
    "        p <- p + coord_cartesian(ylim = c(0, 1))\n",
    "    }\n",
    "        \n",
    "    if(hide_legend) {\n",
    "        p <- p + theme(legend.position=\"none\")\n",
    "    }\n",
    "    \n",
    "    return(p)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_prior['maxs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['maxs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG: predictions without fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = GPflow.kernels.Linear(1)\n",
    "p = GPflow.kernels.PeriodicKernel(1)\n",
    "r = GPflow.kernels.RBF(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.predict(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(31, 365*4, int(365*4-31+1))[:,None]\n",
    "\n",
    "model = GPflow.gpr.GPR(X, Y, kern = kernel)\n",
    "\n",
    "l.predict_y(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
